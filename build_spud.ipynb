{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.getcwd() + '/src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from morph_dict_tools.universal import *\n",
    "from ud_tools import *\n",
    "from syntactic_patterns import *\n",
    "from generate_data_multiprocess import parallel_replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building SPUD treebanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify here the languages you want to build the spud for\n",
    "# languages\n",
    "langs = [\"ar\", \"de\", \"en\", \"fr\",\"ru\"] # [\"en\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the POS tags you want to include in the perturbation\n",
    "upos_filter = [\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\", \"ADV\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "After you ran the preparation script, you should have downloaded and preprocessed the UD treebanks, and have the morphological dictionaries available as pickles. \n",
    "\n",
    "First, let's load the morphological dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load morphdicts\n"
     ]
    }
   ],
   "source": [
    "print('load morphdicts')\n",
    "morphdicts = {lang: load_morphdict_from_pickle(lang) for lang in langs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load the treebanks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_path_mod = \"data/ud-mod/\"\n",
    "tb_path_orig = \"data/ud-treebanks-v2.10/\"\n",
    "tb_paths = {\n",
    "    \"ar\": {\n",
    "        \"train\": tb_path_mod + \"UD_Arabic-PADT/ar_padt-ud-train.conllu\",\n",
    "        \"dev\": tb_path_mod + \"UD_Arabic-PADT/ar_padt-ud-dev.conllu\",\n",
    "        \"test\": tb_path_mod + \"UD_Arabic-PADT/ar_padt-ud-test.conllu\"},\n",
    "    \"de\": {\n",
    "        \"train\": tb_path_mod + \"UD_German-HDT/de_hdt-ud-train.conllu\", \n",
    "        \"dev\": tb_path_mod + \"UD_German-HDT/de_hdt-ud-dev.conllu\", \n",
    "        \"test\": tb_path_mod + \"UD_German-HDT/de_hdt-ud-test.conllu\"},\n",
    "    \"en\": {\n",
    "        \"train\": tb_path_mod + \"UD_English-EWT/en_ewt-ud-train.conllu\",\n",
    "        \"dev\":   tb_path_mod + \"UD_English-EWT/en_ewt-ud-dev.conllu\",\n",
    "        \"test\":  tb_path_mod + \"UD_English-EWT/en_ewt-ud-test.conllu\"},\n",
    "    \"fr\": {\n",
    "        \"train\": tb_path_mod + \"UD_French-GSD/fr_gsd-ud-train.conllu\",\n",
    "        \"dev\": tb_path_mod + \"UD_French-GSD/fr_gsd-ud-dev.conllu\",\n",
    "        \"test\": tb_path_mod + \"UD_French-GSD/fr_gsd-ud-test.conllu\"},\n",
    "    \"ru\": {\n",
    "        \"train\": tb_path_orig + \"UD_Russian-SynTagRus/ru_syntagrus-ud-train.conllu\",\n",
    "        \"dev\": tb_path_orig + \"UD_Russian-SynTagRus/ru_syntagrus-ud-dev.conllu\",\n",
    "        \"test\": tb_path_orig + \"UD_Russian-SynTagRus/ru_syntagrus-ud-test.conllu\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are loading the dev and test splits: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load treebanks for  ar\n",
      "read file  data/ud-mod/UD_Arabic-PADT/ar_padt-ud-dev.conllu\n",
      "parse data into token lists\n",
      "apply cutoff of  None\n",
      "convert token lists to trees\n",
      "Done parsing\n",
      "read file  data/ud-mod/UD_Arabic-PADT/ar_padt-ud-test.conllu\n",
      "parse data into token lists\n",
      "apply cutoff of  None\n",
      "convert token lists to trees\n",
      "Done parsing\n",
      "load treebanks for  de\n",
      "read file  data/ud-mod/UD_German-HDT/de_hdt-ud-dev.conllu\n",
      "parse data into token lists\n",
      "apply cutoff of  None\n",
      "convert token lists to trees\n",
      "Done parsing\n",
      "read file  data/ud-mod/UD_German-HDT/de_hdt-ud-test.conllu\n",
      "parse data into token lists\n",
      "apply cutoff of  None\n",
      "convert token lists to trees\n",
      "Done parsing\n",
      "load treebanks for  en\n",
      "read file  data/ud-mod/UD_English-EWT/en_ewt-ud-dev.conllu\n",
      "parse data into token lists\n",
      "apply cutoff of  None\n",
      "convert token lists to trees\n",
      "Done parsing\n",
      "read file  data/ud-mod/UD_English-EWT/en_ewt-ud-test.conllu\n",
      "parse data into token lists\n",
      "apply cutoff of  None\n",
      "convert token lists to trees\n",
      "Done parsing\n",
      "load treebanks for  fr\n",
      "read file  data/ud-mod/UD_French-GSD/fr_gsd-ud-dev.conllu\n",
      "parse data into token lists\n",
      "apply cutoff of  None\n",
      "convert token lists to trees\n",
      "Done parsing\n",
      "read file  data/ud-mod/UD_French-GSD/fr_gsd-ud-test.conllu\n",
      "parse data into token lists\n",
      "apply cutoff of  None\n",
      "convert token lists to trees\n",
      "Done parsing\n",
      "load treebanks for  ru\n",
      "read file  data/ud-treebanks-v2.10/UD_Russian-SynTagRus/ru_syntagrus-ud-dev.conllu\n",
      "parse data into token lists\n",
      "apply cutoff of  None\n",
      "convert token lists to trees\n",
      "Done parsing\n",
      "read file  data/ud-treebanks-v2.10/UD_Russian-SynTagRus/ru_syntagrus-ud-test.conllu\n",
      "parse data into token lists\n",
      "apply cutoff of  None\n",
      "convert token lists to trees\n",
      "Done parsing\n"
     ]
    }
   ],
   "source": [
    "treebanks = dict() \n",
    "for lang in langs:\n",
    "    print('load treebanks for ', lang)\n",
    "    treebanks[lang] = {\n",
    "        \"dev\": load_ud_treebank(tb_paths[lang][\"dev\"]),\n",
    "        \"test\": load_ud_treebank(tb_paths[lang][\"test\"])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each language, load the syntactic patterns which contain the syntactic contexts for replacements\n",
    "syntactic_patterns = dict()\n",
    "for lang in langs:\n",
    "    pattern_config = pattern_configs[lang]\n",
    "    dev_test_trees = treebanks[lang][\"dev\"][1] + treebanks[lang][\"test\"][1]\n",
    "    syntactic_patterns[lang] = SyntacticPatterns(dev_test_trees, upos_filter=upos_filter, pattern_config=pattern_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop making sense!\n",
    "\n",
    "Now we are ready to build the SPUD treebanks. \n",
    "For this, we need to define the following parameters:\n",
    "- The number of nonce versions we want to build per sentence (num_runs)\n",
    "- The number of sentences we want to build, since this might take some time (cutoff)\n",
    "- (optional, default=1) The number of parallel cpu processes we want to use. This depends on available RAM and language, since the morphological dictionaries per language have different sizes. The numbers here are fit for 64 GB RAM + SWAP, so decrease them if you have less RAM available (lang2processes)\n",
    "\n",
    "The cell below generates SPUD test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang2run2newsents = {lang:dict() for lang in langs}\n",
    "# generate num_runs versions. This might take a while, you might want to specify a cutoff for the number of sentences to generate\n",
    "num_runs = 3\n",
    "cutoff = 100\n",
    "split = \"dev\"\n",
    "lang2processes = {\n",
    "    \"ar\": 2,\n",
    "    \"de\": 6,\n",
    "    \"en\": 3,\n",
    "    \"fr\": 2,\n",
    "    \"ru\": 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace tokens in dev ar\n",
      "run  0\n",
      "num_processes 1\n",
      "treebank size 100\n",
      "slice_size 101\n",
      "build and start processes\n",
      "i 0 start 0 end 100\n",
      "0, join processes\n",
      "join process 0\n",
      "Created  100  new sentences\n",
      "Saving sentences to conllu file\n",
      "Done with slice  0\n",
      "done creating. Now reloading and merging\n",
      "0\n",
      "read file  _ar_sents_slice_0.conllu\n",
      "parse data into token lists\n",
      "apply cutoff of  None\n",
      "remove tmp files\n",
      "run  1\n",
      "num_processes 1\n",
      "treebank size 100\n",
      "slice_size 101\n",
      "build and start processes\n",
      "i 0 start 0 end 100\n",
      "0, join processes\n",
      "join process 0\n",
      "Created  100  new sentences\n",
      "Saving sentences to conllu file\n",
      "Done with slice  0\n",
      "done creating. Now reloading and merging\n",
      "0\n",
      "read file  _ar_sents_slice_0.conllu\n",
      "parse data into token lists\n",
      "apply cutoff of  None\n",
      "remove tmp files\n",
      "run  2\n",
      "num_processes 1\n",
      "treebank size 100\n",
      "slice_size 101\n",
      "build and start processes\n",
      "i 0 start 0 end 100\n",
      "0, join processes\n",
      "join process 0\n",
      "Created  100  new sentences\n",
      "Saving sentences to conllu file\n",
      "Done with slice  0\n",
      "done creating. Now reloading and merging\n",
      "0\n",
      "read file  _ar_sents_slice_0.conllu\n",
      "parse data into token lists\n",
      "apply cutoff of  None\n",
      "remove tmp files\n",
      "replace tokens in dev de\n",
      "run  0\n",
      "num_processes 1\n",
      "treebank size 100\n",
      "slice_size 101\n",
      "build and start processes\n",
      "i 0 start 0 end 100\n",
      "0, join processes\n",
      "join process 0\n",
      "Created  100  new sentences\n",
      "Saving sentences to conllu file\n",
      "Done with slice  0\n",
      "done creating. Now reloading and merging\n",
      "0\n",
      "read file  _de_sents_slice_0.conllu\n",
      "parse data into token lists\n",
      "apply cutoff of  None\n",
      "remove tmp files\n",
      "run  1\n",
      "num_processes 1\n",
      "treebank size 100\n",
      "slice_size 101\n",
      "build and start processes\n",
      "i 0 start 0 end 100\n",
      "0, join processes\n",
      "join process 0\n",
      "Created  100  new sentences\n",
      "Saving sentences to conllu file\n",
      "Done with slice  0\n",
      "done creating. Now reloading and merging\n",
      "0\n",
      "read file  _de_sents_slice_0.conllu\n",
      "parse data into token lists\n",
      "apply cutoff of  None\n",
      "remove tmp files\n",
      "run  2\n",
      "num_processes 1\n",
      "treebank size 100\n",
      "slice_size 101\n",
      "build and start processes\n",
      "i 0 start 0 end 100\n",
      "0, join processes\n",
      "join process 0\n",
      "Created  100  new sentences\n",
      "Saving sentences to conllu file\n",
      "Done with slice  0\n",
      "done creating. Now reloading and merging\n",
      "0\n",
      "read file  _de_sents_slice_0.conllu\n",
      "parse data into token lists\n",
      "apply cutoff of  None\n",
      "remove tmp files\n",
      "replace tokens in dev en\n",
      "run  0\n",
      "num_processes 1\n",
      "treebank size 100\n",
      "slice_size 101\n",
      "build and start processes\n",
      "i 0 start 0 end 100\n",
      "0, Created  100  new sentences\n",
      "Saving sentences to conllu file\n",
      "Done with slice  0\n",
      "join processes\n",
      "join process 0\n",
      "done creating. Now reloading and merging\n",
      "0\n",
      "read file  _en_sents_slice_0.conllu\n",
      "parse data into token lists\n",
      "apply cutoff of  None\n",
      "remove tmp files\n",
      "run  1\n",
      "num_processes 1\n",
      "treebank size 100\n",
      "slice_size 101\n",
      "build and start processes\n",
      "i 0 start 0 end 100\n",
      "0, Created  100  new sentences\n",
      "Saving sentences to conllu file\n",
      "Done with slice  0\n",
      "join processes\n",
      "join process 0\n",
      "done creating. Now reloading and merging\n",
      "0\n",
      "read file  _en_sents_slice_0.conllu\n",
      "parse data into token lists\n",
      "apply cutoff of  None\n",
      "remove tmp files\n",
      "run  2\n",
      "num_processes 1\n",
      "treebank size 100\n",
      "slice_size 101\n",
      "build and start processes\n",
      "i 0 start 0 end 100\n",
      "0, Created  100  new sentences\n",
      "Saving sentences to conllu file\n",
      "Done with slice  0\n",
      "join processes\n",
      "join process 0\n",
      "done creating. Now reloading and merging\n",
      "0\n",
      "read file  _en_sents_slice_0.conllu\n",
      "parse data into token lists\n",
      "apply cutoff of  None\n",
      "remove tmp files\n",
      "replace tokens in dev fr\n",
      "run  0\n",
      "num_processes 1\n",
      "treebank size 100\n",
      "slice_size 101\n",
      "build and start processes\n",
      "i 0 start 0 end 100\n",
      "0, Created  100  new sentences\n",
      "Saving sentences to conllu file\n",
      "Done with slice  0\n",
      "join processes\n",
      "join process 0\n",
      "done creating. Now reloading and merging\n",
      "0\n",
      "read file  _fr_sents_slice_0.conllu\n",
      "parse data into token lists\n",
      "apply cutoff of  None\n",
      "remove tmp files\n",
      "run  1\n",
      "num_processes 1\n",
      "treebank size 100\n",
      "slice_size 101\n",
      "build and start processes\n",
      "i 0 start 0 end 100\n",
      "0, Created  100  new sentences\n",
      "Saving sentences to conllu file\n",
      "Done with slice  0\n",
      "join processes\n",
      "join process 0\n",
      "done creating. Now reloading and merging\n",
      "0\n",
      "read file  _fr_sents_slice_0.conllu\n",
      "parse data into token lists\n",
      "apply cutoff of  None\n",
      "remove tmp files\n",
      "run  2\n",
      "num_processes 1\n",
      "treebank size 100\n",
      "slice_size 101\n",
      "build and start processes\n",
      "i 0 start 0 end 100\n",
      "0, Created  100  new sentences\n",
      "Saving sentences to conllu file\n",
      "Done with slice  0\n",
      "join processes\n",
      "join process 0\n",
      "done creating. Now reloading and merging\n",
      "0\n",
      "read file  _fr_sents_slice_0.conllu\n",
      "parse data into token lists\n",
      "apply cutoff of  None\n",
      "remove tmp files\n",
      "replace tokens in dev ru\n",
      "run  0\n",
      "num_processes 1\n",
      "treebank size 100\n",
      "slice_size 101\n",
      "build and start processes\n",
      "i 0 start 0 end 100\n",
      "0, join processes\n",
      "join process 0\n",
      "Created  100  new sentences\n",
      "Saving sentences to conllu file\n",
      "Done with slice  0\n",
      "done creating. Now reloading and merging\n",
      "0\n",
      "read file  _ru_sents_slice_0.conllu\n",
      "parse data into token lists\n",
      "apply cutoff of  None\n",
      "remove tmp files\n",
      "run  1\n",
      "num_processes 1\n",
      "treebank size 100\n",
      "slice_size 101\n",
      "build and start processes\n",
      "i 0 start 0 end 100\n",
      "0, join processes\n",
      "join process 0\n",
      "Created  100  new sentences\n",
      "Saving sentences to conllu file\n",
      "Done with slice  0\n",
      "done creating. Now reloading and merging\n",
      "0\n",
      "read file  _ru_sents_slice_0.conllu\n",
      "parse data into token lists\n",
      "apply cutoff of  None\n",
      "remove tmp files\n",
      "run  2\n",
      "num_processes 1\n",
      "treebank size 100\n",
      "slice_size 101\n",
      "build and start processes\n",
      "i 0 start 0 end 100\n",
      "0, join processes\n",
      "join process 0\n",
      "Created  100  new sentences\n",
      "Saving sentences to conllu file\n",
      "Done with slice  0\n",
      "done creating. Now reloading and merging\n",
      "0\n",
      "read file  _ru_sents_slice_0.conllu\n",
      "parse data into token lists\n",
      "apply cutoff of  None\n",
      "remove tmp files\n"
     ]
    }
   ],
   "source": [
    "lang2run2newsents = {lang:dict() for lang in langs}\n",
    "\n",
    "for lang in langs:\n",
    "    num_processes = 1 # lang2processes[lang]\n",
    "    print('replace tokens in dev', lang)\n",
    "    for i in range(num_runs):\n",
    "        print('run ', i)\n",
    "        new_sents = parallel_replacement(\n",
    "            lang=lang,\n",
    "            sents=treebanks[lang][split][0][:cutoff],\n",
    "            trees=treebanks[lang][split][1][:cutoff],\n",
    "            morphdict=morphdicts[lang],\n",
    "            synt_patterns=syntactic_patterns[lang],\n",
    "            upos_filter=upos_filter,\n",
    "            num_processes=num_processes)\n",
    "        lang2run2newsents[lang][i] = new_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write ru run 2\r"
     ]
    }
   ],
   "source": [
    "out_dir_prefix = \"data/spud/\"\n",
    "for lang in langs:\n",
    "    for r in range(num_runs):\n",
    "        # print state with flush\n",
    "        print(f\"write {lang} run {r}\", end=\"\\r\", flush=True)\n",
    "        out_dir = f\"{out_dir_prefix}{lang}/{r}/\"\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        new_sents = lang2run2newsents[lang][r]\n",
    "        serialize_sents_to_conllu_file(new_sents, f\"{out_dir}spud_dev.conllu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending to a new language\n",
    "\n",
    "In principle, you need to be able to execute all steps in the above cells for the new language. This requires the following steps\n",
    "\n",
    "- Implementing a Morphological Dictionary for this language\n",
    "    - Understand the class for an existing one (e.g. `src/morph_dict_tools/udlex_french.py` is a good example) and adapt the method for preparing the dict from file with the UDLexicon of your language.\n",
    "    - Then create a pickle of the morphdict by extending `prep/pickle_morphdicts.py` with the class of your new dictionary. \n",
    "- Add the treebank files to the paths as in this notebook above, and a two-letter language id to the list `langs`\n",
    "- Add a syntactic pattern to the pattern config in `src/syntactic_patterns.py` (Documentation is provided there)\n",
    "- And that's it! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
