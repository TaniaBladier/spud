{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.getcwd() + '/src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip3 install conllu \n",
    "#! pip3 install wiktextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from morph_dict_tools.universal import *\n",
    "from ud_tools import *\n",
    "from syntactic_patterns import *\n",
    "from generate_data_multiprocess import parallel_replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building SPUD treebanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify here the languages you want to build the spud for\n",
    "# languages\n",
    "#langs = [\"ar\", \"de\", \"en\", \"fr\",\"ru\"] # [\"en\"] \n",
    "langs = [\"fr\"] # [\"en\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the POS tags you want to include in the perturbation\n",
    "upos_filter = [\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\", \"ADV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read and preprocess apertium\n",
      "read and preprocess lefff\n",
      "ap   :  (156300, 5)\n",
      "lefff:  (649587, 5)\n",
      "concatenate both and create feat dicts\n",
      "merge apertium and lefff\n",
      "french:  (805884, 5)\n",
      "turn dataframe into dict. Might take a few minutes\n",
      "Number of entries to convert:  805884\n",
      "0\n",
      "250000\n",
      "500000\n",
      "750000\n",
      "loading muted h words\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from morph_dict_tools.udlex_french import FrenchMorphDict\n",
    "import pickle\n",
    "\n",
    "morph_dict = FrenchMorphDict()\n",
    "with open('data/morph/pickles/FrenchMorphDict.pickle', 'wb') as f:\n",
    "    pickle.dump(morph_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "After you ran the preparation script, you should have downloaded and preprocessed the UD treebanks, and have the morphological dictionaries available as pickles. \n",
    "\n",
    "First, let's load the morphological dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load morphdicts\n"
     ]
    }
   ],
   "source": [
    "print('load morphdicts')\n",
    "morphdicts = {lang: load_morphdict_from_pickle(lang) for lang in langs}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load the treebanks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tb_path_mod = \"data/ud-mod/\"\n",
    "#tb_path_orig = \"data/ud-treebanks-v2.10/\"\n",
    "\"\"\"\n",
    "tb_paths = {\n",
    "    \"ar\": {\n",
    "        \"train\": tb_path_mod + \"UD_Arabic-PADT/ar_padt-ud-train.conllu\",\n",
    "        \"dev\": tb_path_mod + \"UD_Arabic-PADT/ar_padt-ud-dev.conllu\",\n",
    "        \"test\": tb_path_mod + \"UD_Arabic-PADT/ar_padt-ud-test.conllu\"},\n",
    "    \"de\": {\n",
    "        \"train\": tb_path_mod + \"UD_German-HDT/de_hdt-ud-train.conllu\", \n",
    "        \"dev\": tb_path_mod + \"UD_German-HDT/de_hdt-ud-dev.conllu\", \n",
    "        \"test\": tb_path_mod + \"UD_German-HDT/de_hdt-ud-test.conllu\"},\n",
    "    \"en\": {\n",
    "        \"train\": tb_path_mod + \"UD_English-EWT/en_ewt-ud-train.conllu\",\n",
    "        \"dev\":   tb_path_mod + \"UD_English-EWT/en_ewt-ud-dev.conllu\",\n",
    "        \"test\":  tb_path_mod + \"UD_English-EWT/en_ewt-ud-test.conllu\"},\n",
    "    \"fr\": {\n",
    "        \"train\": tb_path_mod + \"UD_French-GSD/fr_gsd-ud-train.conllu\",\n",
    "        \"dev\": tb_path_mod + \"UD_French-GSD/fr_gsd-ud-dev.conllu\",\n",
    "        \"test\": tb_path_mod + \"UD_French-GSD/fr_gsd-ud-test.conllu\"},\n",
    "    \"ru\": {\n",
    "        \"train\": tb_path_orig + \"UD_Russian-SynTagRus/ru_syntagrus-ud-train.conllu\",\n",
    "        \"dev\": tb_path_orig + \"UD_Russian-SynTagRus/ru_syntagrus-ud-dev.conllu\",\n",
    "        \"test\": tb_path_orig + \"UD_Russian-SynTagRus/ru_syntagrus-ud-test.conllu\"},\n",
    "}\n",
    "\"\"\"\n",
    "tb_paths = {\n",
    "    \"fr\": {\n",
    "        \"train\": \"gutenberg_dumas.test.conll\",\n",
    "        \"dev\": \"gutenberg_dumas.test.conll\",\n",
    "        \"test\": \"gutenberg_dumas.test.conll\"},\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are loading the dev and test splits: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load treebanks for  fr\n",
      "read file  gutenberg_dumas.test.conll\n",
      "parse data into token lists\n",
      "apply cutoff of  None\n",
      "convert token lists to trees\n",
      "Done parsing\n",
      "read file  gutenberg_dumas.test.conll\n",
      "parse data into token lists\n",
      "apply cutoff of  None\n",
      "convert token lists to trees\n",
      "Done parsing\n"
     ]
    }
   ],
   "source": [
    "treebanks = dict() \n",
    "for lang in langs:\n",
    "    print('load treebanks for ', lang)\n",
    "    treebanks[lang] = {\n",
    "        \"dev\": load_ud_treebank(tb_paths[lang][\"dev\"]),\n",
    "        \"test\": load_ud_treebank(tb_paths[lang][\"test\"])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each language, load the syntactic patterns which contain the syntactic contexts for replacements\n",
    "syntactic_patterns = dict()\n",
    "for lang in langs:\n",
    "    pattern_config = pattern_configs[lang]\n",
    "    dev_test_trees = treebanks[lang][\"dev\"][1] + treebanks[lang][\"test\"][1]\n",
    "    syntactic_patterns[lang] = SyntacticPatterns(dev_test_trees, upos_filter=upos_filter, pattern_config=pattern_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop making sense!\n",
    "\n",
    "Now we are ready to build the SPUD treebanks. \n",
    "For this, we need to define the following parameters:\n",
    "- The number of nonce versions we want to build per sentence (num_runs)\n",
    "- The number of sentences we want to build, since this might take some time (cutoff)\n",
    "- (optional, default=1) The number of parallel cpu processes we want to use. This depends on available RAM and language, since the morphological dictionaries per language have different sizes. The numbers here are fit for 64 GB RAM + SWAP, so decrease them if you have less RAM available (lang2processes)\n",
    "\n",
    "The cell below generates SPUD test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang2run2newsents = {lang:dict() for lang in langs}\n",
    "# generate num_runs versions. \n",
    "#This might take a while, you might want to specify a cutoff for the number of sentences to generate\n",
    "num_runs = 3\n",
    "cutoff = 300000\n",
    "split = \"dev\"\n",
    "lang2processes = {\n",
    "    \"fr\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29768\n"
     ]
    }
   ],
   "source": [
    "print(len(treebanks['fr'][split][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace tokens in dev fr\n",
      "run  0\n",
      "num_processes 1\n",
      "treebank size 29768\n",
      "slice_size 29769\n",
      "build and start processes\n",
      "i 0 start 0 end 29768\n"
     ]
    }
   ],
   "source": [
    "lang2run2newsents = {lang:dict() for lang in langs}\n",
    "\n",
    "\n",
    "for lang in langs:\n",
    "    num_processes = 1 # lang2processes[lang]\n",
    "    print('replace tokens in dev', lang)\n",
    "    for i in range(num_runs):\n",
    "        print('run ', i)\n",
    "        try:\n",
    "            new_sents = parallel_replacement(\n",
    "                lang=lang,\n",
    "                sents=treebanks[lang][split][0][:cutoff], #[12845:12850], #cutoff\n",
    "                trees=treebanks[lang][split][1][:cutoff],#cutoff\n",
    "                morphdict=morphdicts[lang],\n",
    "                synt_patterns=syntactic_patterns[lang],\n",
    "                upos_filter=upos_filter,\n",
    "                num_processes=num_processes)\n",
    "            lang2run2newsents[lang][i] = new_sents\n",
    "        except:\n",
    "            pass\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/spud/fr/0/\n"
     ]
    }
   ],
   "source": [
    "out_dir_prefix = \"data/spud/\"\n",
    "for lang in langs:\n",
    "    for r in range(num_runs):\n",
    "        # print state with flush\n",
    "        print(f\"write {lang} run {r}\", end=\"\\r\", flush=True)\n",
    "        out_dir = f\"{out_dir_prefix}{lang}/{r}/\"\n",
    "        print(out_dir)\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        new_sents = lang2run2newsents[lang][r]\n",
    "        serialize_sents_to_conllu_file(new_sents, f\"{out_dir}spud_dev.conllu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending to a new language\n",
    "\n",
    "In principle, you need to be able to execute all steps in the above cells for the new language. This requires the following steps\n",
    "\n",
    "- Implementing a Morphological Dictionary for this language\n",
    "    - Understand the class for an existing one (e.g. `src/morph_dict_tools/udlex_french.py` is a good example) and adapt the method for preparing the dict from file with the UDLexicon of your language.\n",
    "    - Then create a pickle of the morphdict by extending `prep/pickle_morphdicts.py` with the class of your new dictionary. \n",
    "- Add the treebank files to the paths as in this notebook above, and a two-letter language id to the list `langs`\n",
    "- Add a syntactic pattern to the pattern config in `src/syntactic_patterns.py` (Documentation is provided there)\n",
    "- And that's it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
